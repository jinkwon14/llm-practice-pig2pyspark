{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Nomic GPR4ALLEMBEDDINGS? \n",
    "# - store to vector database \n",
    "\n",
    "# Vector databases are used in Low-Latency Machine Applications (LLMs) to provide additional information that LLMs have not been trained on. \n",
    "# - TODO: can we use previously successful set of (pig_code, pyspark_code, sample_data), store them to vector DB and use that for future code gen? \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initial Setup \n",
    "\n",
    "* generate LangSmith API key.\n",
    "* TODO: How to safely save and load API keys\n",
    "* https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracing progress\n",
    "\n",
    "import os \n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure \n",
    "run_local = \"Yes\"\n",
    "# local_llm = \"mistral\" # mistral: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n",
    "# local_llm = \"mixtral\"  # mixtral: https://scalastic.io/en/mixtral-ollama-llamaindex-llm/\n",
    "local_llm = \"llama3\" # llama3: https://python.langchain.com/docs/integrations/chat/ollama/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['docker-compose.yml', 'Dockerfile', 'file_handler.py', 'faq_generator.ipynb', 'setup-env.sh', 'pig2pyspark_generator.ipynb', '.Trash-0', 'requirements.txt', 'langgraph2.ipynb', 'coding', 'scripts', '.ipynb_checkpoints', '.gitignore', 'output', 'data', '.cache', '.git']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the list of all files and directories in the current working directory\n",
    "files_and_directories = os.listdir()\n",
    "\n",
    "print(files_and_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Index?) - Uplaod Supporting Documents \n",
    "Load data: https://python.langchain.com/docs/integrations/document_loaders/recursive_url/\n",
    "\n",
    "We will pull AWS EC2 docs: https://docs.aws.amazon.com/ec2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "# from langchain_community.document_loaders import WebBaseLoader # this is for pulling \n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# # from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "# Load\n",
    "url = \"https://help.netflix.com/en/node/102377\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=5, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and index\n",
    "if run_local == \"Yes\":\n",
    "    embedding = GPT4AllEmbeddings()\n",
    "else:\n",
    "    # embedding = MistralAIEmbeddings(mistral_api_key=mistral_api_key)\n",
    "    pass\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample HTML from \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage you want to fetch\n",
    "url = 'https://network.mobile.rakuten.co.jp/faq/detail/00001238/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Print out the prettified HTML\n",
    "    reference_webpate_html = soup.prettify()\n",
    "    print(reference_webpate_html)\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "display(HTML(reference_webpate_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph - LLMs \n",
    "\n",
    "We build two LLMs: \n",
    "* Fetch Answer either from vector DB or webbase\n",
    "* Put result in HTML format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# we use locally hosted llm models \n",
    "llm = ChatOllama(model='llama3', format=\"json\", temperature=0.4)\n",
    "\n",
    "# reference: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "question = \"How do I get started with Copilot?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt_data_gen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a web developer fluent in HTML and knowledgeable software engineer.\n",
    "    \n",
    "    Question: \\n\\n Use document as a reference to craft a response to a question in HTML format. Insert plots or flowcharts whenever needed. Question is \"{question}\" \\n\\n\n",
    "    Context: \\n\\n {context} \\n\\n \n",
    "\n",
    "    Please output only HTML code.\n",
    "    \"\"\",\n",
    "    \n",
    "    input_variables=[\"context\", \"ref_html\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "# LLM\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, temperature=0.3)\n",
    "else:\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
    "    )\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt_data_gen | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"ref_html\": reference_webpate_html , \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_code = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"utf-8\" />\n",
    "    <meta name=\"description\" content=\"GitHub Copilot Chat can help you by providing answers to coding related questions  directly within a supported IDE.\" />\n",
    "    <title>About GitHub Copilot Chat in your IDE - GitHub Docs</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>About GitHub Copilot Chat in your IDE - GitHub Docs</h1>\n",
    "    <p>Getting started with GitHub Copilot - GitHub DocsSkip to main contentGitHub DocsVersion: Free, Pro, &amp; TeamSearch GitHub DocsGitHub Copilot/Use GitHub Copilot/Getting startedHomeGitHub CopilotAbout GitHub CopilotQuickstartUse GitHub CopilotGetting startedFinding matching codeCopilot IndividualAbout GitHub Copilot IndividualCopilot Individual feature setCopilot BusinessAbout GitHub Copilot BusinessCopilot Business feature setEnabling GitHub Copilot BusinessCopilot EnterpriseOverviewAbout Copilot EnterpriseCopilot Enterprise feature setCopilot Chat in GitHub.comAbout Copilot Chat in GitHub.comCopilot pull request summariesAbout PR summariesManage Copilot in your organizationManaging accessManaging policiesExcluding contentAudit logsCopilot ChatAbout Copilot Chat (Mobile)Enabling Copilot Chat (Mobile)Use Copilot Chat (Mobile)About Copilot Chat (IDE)Use Copilot Chat (IDE)Copilot in the CLIAbout Copilot in the CLISetting up Copilot in the CLIUsing Copilot in the CLIConfiguring Copilot in the CLIConfigure GitHub CopilotGitHub.comIn your environmentNetwork settingsTroubleshootingCommon issues with GitHub CopilotView logsFirewall settingsNetwork errorsCopilot Chat</p>\n",
    "    <h1>Getting started with GitHub Copilot</h1>\n",
    "    <p>You can start using GitHub Copilot by installing the extension in your preferred environment.</p>\n",
    "    <p>Who can use this feature?</p>\n",
    "    <p>GitHub Copilot can be managed through personal accounts with GitHub Copilot Individual or through organization accounts with GitHub Copilot Business.</p>\n",
    "    <h1>About GitHub Copilot Business</h1>\n",
    "    <p>With GitHub Copilot Business you can manage access to GitHub Copilot for your organization.</p>\n",
    "    <p>Who can use this feature?</p>\n",
    "    <p>GitHub Copilot can be managed through personal accounts with GitHub Copilot Individual or through organization accounts with GitHub Copilot Business.</p>\n",
    "    <p>In this article</p>\n",
    "    <p>About Copilot Business</p>\n",
    "    <p>Enabling and setting up Copilot Business</p>\n",
    "    <p>About billing for Copilot Business</p>\n",
    "    <p>Requesting or granting access to Copilot</p>\n",
    "    <p>Further reading</p>\n",
    "    <p>Get GitHub Copilot Business</p>\n",
    "    <p>About Copilot Business</p>\n",
    "    <p>GitHub Copilot is an AI-powered coding assistant that helps developers write code faster.</p>\n",
    "    <p>With Copilot Business, you can manage access to GitHub Copilot for organizations. Once you grant an organization access to GitHub Copilot, the administrators of that organization can grant access to individuals and teams. For more information, see \"Enabling and setting up GitHub Copilot Business.\"</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two templates: \n",
    "# 1. pig code to benchmark input data\n",
    "prompt_data_gen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and Python coding languages.\n",
    "    Generate Python code that do the following: \n",
    "    1. Generate 20 lines or more CSV data that can be used to test the PIG code. \n",
    "       Ensure column names are consistent with the names in PIG code. \n",
    "    2. Write Python code that save this CSV data to the directory provided. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "    Here is the directory to save CSV file: \\n\\n {sample_input_path} \\n\\n\n",
    "\n",
    "    Give a string of Python code with correct indentation that can be ran to create and save CSV file to correct path. \n",
    "    Provide this as a JSON with a single key 'data_gen_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\", \"sample_input_path\"],\n",
    ")\n",
    "sample_input_code_generator = prompt_data_gen | llm | JsonOutputParser()\n",
    "\n",
    "prompt_data_regen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and Python coding languages.\n",
    "    Generate Python code that do the following: \n",
    "    * Debug and share updated Python code to generate 20 lines or more CSV data that can be used to thest the PIG code. \n",
    "    * Use the error message and the data that resulted in error as a reference to fix the Python code. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "    Here is the Python code with error: \\n\\n {pycode_error} \\n\\n\n",
    "    Here is the Python code error message: \\n\\n {pycode_error_message} \\n\\n\n",
    "    Here is the directory to save CSV file: \\n\\n {sample_input_path} \\n\\n\n",
    "\n",
    "    Give a string of Python code with correct indentation that can be ran to create and save CSV file to correct path. \n",
    "    Provide this as a JSON with a single key 'data_gen_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\", \"pycode_error\", \"pycode_error_message\", \"sample_input_path\"],\n",
    ")\n",
    "fix_sample_input_code_generator = prompt_data_regen | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# 2. pig code to pyspark code \n",
    "prompt_pig2pyspark = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and PySpark coding languages.\n",
    "    Generate PySpark code that do the following: \n",
    "    * Implement same logic and methods as the provided PIG code. \n",
    "    * When ran against a sample input data, outputs identical result as PIG code. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "\n",
    "    Give a string of PySpark code with correct indentation. \n",
    "    Provide this as a JSON with a single key 'pyspark_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\"],\n",
    ")\n",
    "pig_to_pyspark_converter = prompt_pig2pyspark | llm | JsonOutputParser()\n",
    "\n",
    "prompt_pig2pyspark_regen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and PySpark coding languages.\n",
    "    Generate PySpark code that do the following: \n",
    "    * Implement same logic and methods as the provided PIG code. \n",
    "    * Use the PySpark code that returned an error message to update the PySpark code. \n",
    "    * Use the PySpark code error message to update the PySpark code. \n",
    "    * When ran against a sample input data, outputs identical result as PIG code. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "    Here is the PySpark code with error: \\n\\n {pycode_error} \\n\\n\n",
    "    Here is the PySpark code error message: \\n\\n {pycode_error_message} \\n\\n\n",
    "\n",
    "    Give a string of PySpark code with correct indentation. \n",
    "    Provide this as a JSON with a single key 'pyspark_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\", \"pycode_error\", \"pycode_error_message\"],\n",
    ")\n",
    "fix_pig_to_pyspark_converter = prompt_pig2pyspark | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test with sample PIG code \n",
    "# load PIG code\n",
    "pig_script_dir = './scripts/pig1.pig'\n",
    "\n",
    "with open(pig_script_dir, 'r') as file:\n",
    "    sample_pig_code = file.read()\n",
    "\n",
    "print(sample_pig_code)\n",
    "data_output_dir = './data'\n",
    "\n",
    "########################################\n",
    "datagen_code = sample_input_code_generator.invoke({\"pig_code\": sample_pig_code, \"sample_input_path\": data_output_dir})\n",
    "print('*'*88)\n",
    "print(datagen_code['data_gen_code'])\n",
    "print('*'*88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Run `$ollama pull model-name` before using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
