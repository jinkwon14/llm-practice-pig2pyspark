{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Code Converter - Apache Pig to PySpark \n",
    "\n",
    "Tools used: \n",
    "* Langchain: \n",
    "* Ollama: Run LLM locally \n",
    "    * LLM models: llama3, mixtral, etc. (TODO: which models are )\n",
    "* Autogen: LLMs work together to output correct code (code writer & code executor)\n",
    "* LangGraph: Write a pipeline with sequence, conditional routing, and loop\n",
    "* LangChain: monitoring \n",
    "\n",
    "---\n",
    "Log: \n",
    "* 05-07-2024: \n",
    "  * AutoGen + Ollama: \n",
    "    * Ollama + Autogen doc: https://ollama.com/blog/openai-compatibility\n",
    "    * jupyter code executor: https://microsoft.github.io/autogen/docs/topics/code-execution/jupyter-code-executor\n",
    "    * \n",
    "  * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Nomic GPR4ALLEMBEDDINGS? \n",
    "# - store to vector database \n",
    "\n",
    "# Vector databases are used in Low-Latency Machine Applications (LLMs) to provide additional information that LLMs have not been trained on. \n",
    "# - TODO: can we use previously successful set of (pig_code, pyspark_code, sample_data), store them to vector DB and use that for future code gen? \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initial Setup \n",
    "\n",
    "* generate LangSmith API key.\n",
    "* TODO: How to safely save and load API keys\n",
    "* https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure \n",
    "run_local = \"Yes\"\n",
    "\n",
    "# select llm model \n",
    "# local_llm = \"mistral\" # mistral: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n",
    "# local_llm = \"mixtral\"  # mixtral: https://scalastic.io/en/mixtral-ollama-llamaindex-llm/\n",
    "# local_llm = \"llama3\" # llama3: https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "local_llm = \"codellama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT! Make sure to run the below in terminal to start ollama and download LLM model.**  \n",
    "`ollama serve`  \n",
    "`ollama pull {model_name}`\n",
    "\n",
    "TODO: Automate the above (e.g., add to Dockerfile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG (Index?) - Uplaod Supporting Documents \n",
    "Not really needed for this project but as a placeholder add vector DB.   \n",
    "* reference: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb?ref=blog.langchain.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader # this is for pulling \n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "# Load\n",
    "url = \"https://github.com/palantir/pyspark-style-guide\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and index\n",
    "if run_local == \"Yes\":\n",
    "    embedding = GPT4AllEmbeddings()\n",
    "else:\n",
    "    # embedding = MistralAIEmbeddings(mistral_api_key=mistral_api_key)\n",
    "    pass\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLMs\n",
    "\n",
    "We build two sets of LLMs: \n",
    "1. PIG code --> create benchmark input data (if none is provided by user)\n",
    "2. PIG code --> PySpark code\n",
    "\n",
    "**references:** \n",
    "* https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "* JsonOutputParser: https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html\n",
    "* OutputParser: https://medium.com/@larry_nguyen/langchain-101-lesson-3-output-parser-406591b094d7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Generate input data (CSV) given Pig code\n",
    "\n",
    "If the user does not provide benchmark input data for testing the PIG code, we will use LLM to generate a Python script. This script will create and save sample data to the specified folder.\n",
    "\n",
    "We achieve this in the following steps: \n",
    "1. Create an LLM prompt template that outputs a Python script designed to generate and save CSV data based on the given PIG code.\n",
    "2. Execute the LLM to produce the Python script.\n",
    "3. Parse the generated Python script.\n",
    "4. Execute the Python script, which results in the CSV file being saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# we use locally hosted llm models \n",
    "llm = ChatOllama(model = local_llm, format = \"json\", temperature=0.2)\n",
    "\n",
    "\n",
    "## Create two templates: \n",
    "# 1. pig code --> benchmark input data\n",
    "prompt_data_gen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and Python coding languages.\n",
    "    Generate Python code that do the following: \n",
    "    1. Generate 20 lines or more CSV data that can be used to test the PIG code. \n",
    "       Ensure column names are consistent with the names in PIG code. \n",
    "    2. Write Python code that save this CSV data to the directory provided. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "    Here is the directory to save CSV file: \\n\\n {sample_input_path} \\n\\n\n",
    "\n",
    "    Give a string of Python code with correct indentation that can be ran to create and save CSV file to correct path. \n",
    "    Provide this as a JSON with a single key 'data_gen_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\", \"sample_input_path\"],\n",
    ")\n",
    "sample_input_code_generator = prompt_data_gen | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Below codes are work in progress.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there was an error in the outputted Python script for generating CSV file, \n",
    "#  add that error back to LLM and re-generate an updated Python script. \n",
    "prompt_data_regen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and Python coding languages.\n",
    "    Generate Python code that do the following: \n",
    "    * Debug and share updated Python code to generate 100 lines or more CSV data that can be used to thest the PIG code. \n",
    "    * Use the error message and the data that resulted in error as a reference to fix the Python code. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "    Here is the Python code with error: \\n\\n {pycode_error} \\n\\n\n",
    "    Here is the Python code error message: \\n\\n {pycode_error_message} \\n\\n\n",
    "    Here is the directory to save CSV file: \\n\\n {sample_input_path} \\n\\n\n",
    "\n",
    "    Give a string of Python code with correct indentation that can be ran to create and save CSV file with more than 100 records to correct path. \n",
    "    Provide this as a JSON with a single key 'data_gen_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\", \"pycode_error\", \"pycode_error_message\", \"sample_input_path\"],\n",
    ")\n",
    "fix_sample_input_code_generator = prompt_data_regen | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# 2. pig code to pyspark code \n",
    "prompt_pig2pyspark = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and PySpark coding languages.\n",
    "    Generate PySpark code that do the following: \n",
    "    * Implement same logic and methods as the provided PIG code. \n",
    "    * When ran against a sample input data, outputs identical result as PIG code. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "\n",
    "    Give a string of PySpark code with correct indentation. \n",
    "    Provide this as a JSON with a single key 'pyspark_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\"],\n",
    ")\n",
    "pig_to_pyspark_converter = prompt_pig2pyspark | llm | JsonOutputParser()\n",
    "\n",
    "prompt_pig2pyspark_regen = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert data scientist fluent in PIG and PySpark coding languages.\n",
    "    Generate PySpark code that do the following: \n",
    "    * Implement same logic and methods as the provided PIG code. \n",
    "    * Use the PySpark code that returned an error message to update the PySpark code. \n",
    "    * Use the PySpark code error message to update the PySpark code. \n",
    "    * When ran against a sample input data, outputs identical result as PIG code. \n",
    "        \n",
    "    Here is the PIG code: \\n\\n {pig_code} \\n\\n\n",
    "    Here is the PySpark code with error: \\n\\n {pycode_error} \\n\\n\n",
    "    Here is the PySpark code error message: \\n\\n {pycode_error_message} \\n\\n\n",
    "\n",
    "    Give a string of PySpark code with correct indentation. \n",
    "    Provide this as a JSON with a single key 'pyspark_code' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"pig_code\", \"pycode_error\", \"pycode_error_message\"],\n",
    ")\n",
    "fix_pig_to_pyspark_converter = prompt_pig2pyspark | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "## test with sample PIG code \n",
    "pig_script_dir = './scripts/pig1.pig'\n",
    "\n",
    "with open(pig_script_dir, 'r') as file:\n",
    "    sample_pig_code = file.read()\n",
    "\n",
    "print('*'*88)\n",
    "print(\"Pig Code\\n\")\n",
    "print(sample_pig_code)\n",
    "print('*'*88)\n",
    "\n",
    "data_output_dir = './data'\n",
    "\n",
    "datagen_code = sample_input_code_generator.invoke({\"pig_code\": sample_pig_code, \n",
    "                                                   \"sample_input_path\": data_output_dir})\n",
    "print('*'*88)\n",
    "print(\"Python Code to generate sample data:\\n\")\n",
    "\n",
    "print(datagen_code['data_gen_code'])\n",
    "print('*'*88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** \n",
    "* Add unit tests to ensure generated data is useful.\n",
    "* Number of records are incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be handled in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "import regex as re\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pyspark as spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelService:\n",
    "    def __init__(self, model_id=\"mistralai/Mistral-7B-Instruct-v0.2\", # LLM pre-trained model \n",
    "                 device=0, # Use 1st GPU \n",
    "                 max_new_tokens=1000 # output token count \n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the model with the specified parameters.\n",
    "        \"\"\"\n",
    "        self.hf = HuggingFacePipeline.from_model_id(\n",
    "            model_id=model_id,\n",
    "            task=\"text-generation\",\n",
    "            device=device,\n",
    "            pipeline_kwargs={\"max_new_tokens\": max_new_tokens},\n",
    "        )\n",
    "        \n",
    "    def query(self, input_text):\n",
    "        \"\"\"\n",
    "        Sends a custom query to the model and returns the output.\n",
    "        \"\"\"\n",
    "        result = self.hf(input_text)\n",
    "        return result.text if hasattr(result, 'text') else result\n",
    "\n",
    "# Usage\n",
    "%time  lm_service = LanguageModelService()  # Initialize once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load PIG Code (optionsal: Upload sample data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create sample data \n",
    "\n",
    "Note Batch GPU can work: https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create sample test data using LLM (if none provided by user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "query_create_sample_data = f\"\"\"\n",
    "    Given the following PIG code, generate sample CSV data that will produce consistent \n",
    "    results when processed by this PIG code. The PIG code is intended to perform operations \n",
    "    such as filtering, grouping, and aggregation. Ensure that the sample data is diverse enough \n",
    "    to test all parts of the code effectively.\\n\\n\n",
    "    PIG Code:\\n{pig_code}\\n\\n\n",
    "    Write a Python code that will save the sample CSV file to ./data/sample1.csv with a header column. Encapsulate the code between ```. \n",
    "    Make sure there is only one code chunk (```).:\n",
    "\"\"\"\n",
    "\n",
    "response = lm_service.query(query_create_sample_data)\n",
    "print(\"Response from model:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indentation(code):\n",
    "    lines = code.split('\\n')\n",
    "    # Find the first non-empty line to determine the base indentation level\n",
    "    base_indent = None\n",
    "    for line in lines:\n",
    "        stripped_line = line.lstrip()\n",
    "        if stripped_line:\n",
    "            base_indent = len(line) - len(stripped_line)\n",
    "            break\n",
    "\n",
    "    if base_indent is None:\n",
    "        return code  # Return original code if it's all empty lines or no base indent found\n",
    "\n",
    "    # Normalize each line by removing the base indentation\n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        stripped_line = line.lstrip()\n",
    "        if len(line) > base_indent:\n",
    "            normalized_lines.append(line[base_indent:])\n",
    "        else:\n",
    "            normalized_lines.append(stripped_line)\n",
    "\n",
    "    return '\\n'.join(normalized_lines)\n",
    "\n",
    "def parse_python_code_from_text(text):\n",
    "    normalized_text = normalize_indentation(text)\n",
    "    \n",
    "    # Define the pattern to extract code between ```python and ```\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    match = re.search(pattern, normalized_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        code_to_execute = match.group(1)\n",
    "        print(code_to_execute)\n",
    "        return code_to_execute\n",
    "    else:\n",
    "        print(\"No Python code block found.\")\n",
    "\n",
    "def run_pyspark_code(code):\n",
    "    \"\"\"\n",
    "    Executes PySpark code, returns either error message or result DataFrame.\n",
    "    Assumes PySpark session and context are already set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exec(code)\n",
    "        return None, globals().get('sales_summary')  # Assuming 'sales_summary' is the result DataFrame\n",
    "    except Exception as e:\n",
    "        return str(e), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2. LLM may not output working test data. Repeat until correct data is outputtedz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Save sample data and output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Save Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = parse_python_code_from_text(response)\n",
    "run_pyspark_code(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Execute PIG code and save output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pig_script(script_path, data_path):\n",
    "    # Set up the environment variable to point to the directory containing the data file\n",
    "    os.environ['PIG_DATA_PATH'] = data_path\n",
    "\n",
    "    # Execute the Pig script using subprocess, assuming Pig is installed and configured to run in local mode\n",
    "    result = subprocess.run(['pig', '-x', 'local', '-f', script_path], capture_output=True, text=True)\n",
    "    \n",
    "    # Check the results of the Pig script execution\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error occurred during Pig script execution:\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"Pig script executed successfully. Output:\")\n",
    "        print(result.stdout)\n",
    "\n",
    "# Define the path to the Pig script and the directory containing the data file\n",
    "pig_script = './scripts/pig1.pig'\n",
    "csv_data = './data/'\n",
    "\n",
    "# Execute the Pig script\n",
    "run_pig_script(pig_script, csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_latest_file(directory, pattern):\n",
    "    \"\"\"\n",
    "    Returns the path to the latest file in the given directory that matches the pattern.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search in.\n",
    "    pattern (str): The file name pattern to match.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the latest file matching the pattern or None if no file matches.\n",
    "    \"\"\"\n",
    "    # Create the full path pattern\n",
    "    search_pattern = os.path.join(directory, pattern)\n",
    "    \n",
    "    # List all files matching the pattern\n",
    "    files = glob.glob(search_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    # Find the latest file based on last modification time\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "# Example usage\n",
    "directory = './output/high_value_transactions'\n",
    "file_pattern = 'part-*'\n",
    "latest_file = get_latest_file(directory, file_pattern)\n",
    "print(f\"The latest file is: {latest_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the output file created by Pig\n",
    "output_file = latest_file  # Adjust path as needed\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(output_file, names=['depStore', 'total_sales'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.1. LLM transcriber PIG2PySpark\n",
    "# Prompt: \n",
    "# You are an experienced Software Engineer and Machine Learning Engineer fluent in PIG and PySpark coding languages. \n",
    "# Rewrite the following PIG code into PySpark so that they can perform identical tasks and output identical results given same input data. \n",
    "# PIG Code: {pig_code}\n",
    "# A.2. Code Parser: Parse and save PySpark code \n",
    "\n",
    "# B. Sample Data Builder \n",
    "# C. PIG Interpreter \n",
    "# D. PySpark Interpreter \n",
    "\n",
    "# If sample data NOT available (run B.)\n",
    "  # Run PIG code and generate output \n",
    "  # 1. Generate PySpark code from PIG (run A.1) and parse/save the PySpark Code (run A.2)\n",
    "# 2. Run against sample data in a separate module.\n",
    "# 3. Check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LLM Layer - Pig2PySpark \n",
    "* https://medium.com/@yash9439/unleashing-the-power-of-falcon-code-a-comparative-analysis-of-implementation-approaches-803048ce65dc\n",
    "* ref: https://medium.com/@ajay_khanna/leveraging-llama2-0-for-question-answering-on-your-own-data-using-cpu-aa6f75868d2d\n",
    "* ref: https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476\n",
    "* https://wellsr.com/python/fine-tuning-llama2-for-question-answering-tasks/\n",
    "* https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_pig2pyspark = f\"\"\"\n",
    "You are an experienced software and machine learning engineer fluent in both PIG and PySpark. \n",
    "Re-write the following PIG code into PySpark code. \n",
    "Ensure PySpark code is logically identical and output identical results as the provided PIG code. \n",
    "Make sure to only share PySpark in a single code block (inside ```). \n",
    "PIG code: {pig_code}\n",
    "\"\"\"\n",
    "\n",
    "%time response = lm_service.query(prompt_pig2pyspark)\n",
    "print(\"Response from model:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_message = run_pyspark_code(response)\n",
    "print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_fix_error(pyspark_code, error_message, input_data):\n",
    "    \"\"\"\n",
    "    Generate prompt to fix error in PySpark code.\n",
    "    Returns a string with the prompt to fix errors.\n",
    "    \"\"\"\n",
    "    return f\"\"\"There was an error in your PySpark code: {error_message}. Please fix and re-share the full PySpark code with relevant updates inside a single code cell.\n",
    "    \n",
    "    Below is the PySpark code that returned an error: \n",
    "    {pyspark_code}. \n",
    "\n",
    "    Below is the input data: \n",
    "    {input_data}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fix_pyspark_code = build_prompt_fix_error(response, error_message, df)\n",
    "\n",
    "response = lm_service.query(prompt_fix_pyspark_code)\n",
    "print(\"Response from model:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = parse_python_code_from_text(response) # this did not run \n",
    "run_pyspark_code(\"\"\"from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Load the data from a CSV file\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
    "transactions = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/sample1.csv\")\n",
    "\n",
    "# Filter transactions to include only those where the amount is greater than 200\n",
    "high_value_transactions = transactions.filter(F.col(\"amount\") > 200)\n",
    "\n",
    "# Group the transactions by store\n",
    "grouped_by_store = high_value_transactions.groupBy(\"depStore\")\n",
    "\n",
    "# Calculate total and average sales per depStore\n",
    "sales_summary = grouped_by_store.agg(F.sum(\"amount\").alias(\"total_sales\"), F.avg(\"amount\").alias(\"average_sales\"))\n",
    "\n",
    "# Store the summary in a CSV file\n",
    "sales_summary.write.option(\"header\", \"true\").csv(\"output/sales_summary\", mode=\"overwrite\")\n",
    "\n",
    "# Optional: Just for demonstration, store filtered data to another directory\n",
    "high_value_transactions.write.option(\"header\", \"true\").csv(\"output/high_value_transactions\", mode=\"overwrite\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_pig_script(script_path, data_path):\n",
    "    # Set up the environment variable to point to the directory containing the data file\n",
    "    os.environ['PIG_DATA_PATH'] = data_path\n",
    "\n",
    "    # Execute the Pig script using subprocess, assuming Pig is installed and configured to run in local mode\n",
    "    result = subprocess.run(['pig', '-x', 'local', '-f', script_path], capture_output=True, text=True)\n",
    "    \n",
    "    # Check the results of the Pig script execution\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error occurred during Pig script execution:\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"Pig script executed successfully. Output:\")\n",
    "        print(result.stdout)\n",
    "\n",
    "# Define the path to the Pig script and the directory containing the data file\n",
    "pig_script = './scripts/pig1.pig'\n",
    "csv_data = './data/'\n",
    "\n",
    "# Execute the Pig script\n",
    "run_pig_script(pig_script, csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_python_code_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pyspark_code(code):\n",
    "    \"\"\"\n",
    "    Executes PySpark code, returns either error message or result DataFrame.\n",
    "    Assumes PySpark session and context are already set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exec(code)\n",
    "        return None, globals().get('sales_summary')  # Assuming 'sales_summary' is the result DataFrame\n",
    "    except Exception as e:\n",
    "        return str(e), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================================\n",
    "# parse Python code inside the code cell (TODO: How to ensure code is consistently inside the triple back ticks?) \n",
    "# How to loop the code so that it runs until correct code is written? \n",
    "# How does Langchain come into play? \n",
    "# When debugging: 1) provide loaded data head 2) code 3) error message or output if run was successful --> output updated code\n",
    "# ==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model and tokenizer from the cache for use\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "# model = AutoModelForCausalLM.from_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "# # Setup the pipeline with local model and tokenizer\n",
    "# text_generation = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0  # Assuming using the first GPU\n",
    "# )\n",
    "\n",
    "# # Generate text\n",
    "# generated_text = text_generation(\"Sample prompt text goes here\", max_length=50)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = f\"Re-write the following PIG code into PySpark code. Following is the PIG code: \\n {pig_script_code}\"\n",
    "# template = f\"\"\"\n",
    "# You are an intelligent software engineer and machine learning engineer. Re-write the following PIG code into PySpark code. Make sure to only share PySpark code so it's easy to copy and paste. \n",
    "# PIG code: {question}\n",
    "# --------------------------------------------------------------------\n",
    "# PySpark code:\"\"\"\n",
    "\n",
    "# sequences = pipeline(\n",
    "#     template,\n",
    "#     max_length=5000,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test PySpark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_starter(): \n",
    "    \"\"\"\n",
    "    Generate prompt to start transcribing PIG to PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_fix_error(): \n",
    "    \"\"\"\n",
    "    Generate prompt to fix error in PySpark code.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_fix_output(): \n",
    "    \"\"\"\n",
    "    Generate prompt to fix code output mismatch (between result from PIG and PySpark). \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "\n",
    "def run_pyspark_code(code_dir, code_name, sample_data_dir): \n",
    "    # start PySpark server\n",
    "    # run code against test data\n",
    "\n",
    "    if error: \n",
    "        return error_message\n",
    "    else: \n",
    "        return result_df\n",
    "\n",
    "def check_resutls(pig_result_df, pyspark_result_df): \n",
    "    # return True if the results are identical \n",
    "    return is_same \n",
    "\n",
    "\n",
    "# loop until both pyspark code runs fine and output data from PIG and PYSpark are the same\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from tabulate import tabulate\n",
    "\n",
    "def build_prompt_starter(pig_code):\n",
    "    \"\"\"\n",
    "    Generate prompt to start transcribing PIG to PySpark.\n",
    "    Returns a string with the starter prompt.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    I need to convert the following Apache PIG script into Apache PySpark code. \n",
    "    The PySpark code should perform the same tasks and produce identical outputs as the PIG code. \n",
    "    Please ensure that the PySpark code uses DataFrame operations wherever possible and include comments explaining any complex parts or transformations.\n",
    "    \n",
    "    PIG Code: \n",
    "    {pig_code}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pig_output = tabulate(pig_output, headers='keys', tablefmt='psql', showindex=\"never\")\n",
    "pyspark_output = tabulate(pyspark_output, headers='keys', tablefmt='psql', showindex=\"never\")\n",
    "\n",
    "\n",
    "def build_prompt_fix_output(pig_output, pyspark_output):\n",
    "    \"\"\"\n",
    "    Generate prompt to fix code output mismatch between results from PIG and PySpark.\n",
    "    Returns a string with the prompt for fixing output mismatches.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    The outputs between PIG and PySpark do not match. Please adjust the PySpark code so that the output from PySpark code is identical to that from PIG code. \n",
    "    Below are the two outputs with mismatch:\n",
    "\n",
    "    Pig Output (ground truth): \n",
    "    {pig_output}\n",
    "\n",
    "    \n",
    "    PySpark Output: \n",
    "    {pyspark_output}\n",
    "    \"\"\"\n",
    "\n",
    "def check_results(pig_result_df, pyspark_result_df):\n",
    "    \"\"\"\n",
    "    Compare PIG and PySpark DataFrames to check if the results are identical.\n",
    "    \"\"\"\n",
    "    return pig_result_df.subtract(pyspark_result_df).count() == 0 and pyspark_result_df.subtract(pig_result_df).count() == 0\n",
    "\n",
    "\n",
    "# Set up PySpark\n",
    "spark = SparkSession.builder.appName(\"Sales Summary\").getOrCreate()\n",
    "\n",
    "# Initial PySpark code generation using an LLM (not shown here)\n",
    "pyspark_code = \"\"\"\n",
    "# Assume pyspark_code is filled with the initially generated code\n",
    "\"\"\"\n",
    "pig_result_df = spark.createDataFrame(...)  # Assume this is setup elsewhere\n",
    "\n",
    "# Run and refine PySpark code\n",
    "error_message, pyspark_result_df = run_pyspark_code(pyspark_code)\n",
    "while error_message or not check_results(pig_result_df, pyspark_result_df):\n",
    "    if error_message:\n",
    "        prompt = build_prompt_fix_error(error_message)\n",
    "    else:\n",
    "        prompt = build_prompt_fix_output()\n",
    "    # Here you would update `pyspark_code` based on the LLM's output (not shown here)\n",
    "    error_message, pyspark_result_df = run_pyspark_code(pyspark_code)\n",
    "\n",
    "# Results are now fine\n",
    "print(\"PySpark code executed successfully and results match PIG.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess to test out sample codes: \n",
    "# link: https://www.google.com/search?q=Python+application+which+run+PIG+code&rlz=1C1OPNX_enUS1108US1108&oq=Python+application+which+run+PIG+code+&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRigAdIBCTEwNTIzajBqN6gCALACAA&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "# langchain \n",
    "\n",
    "# langsmith \n",
    "\n",
    "# streamlit \n",
    "\n",
    "# airflow \n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
